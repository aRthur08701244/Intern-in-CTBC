{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42d5961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "已找到 model檔。Found model file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import datetime\n",
    "import monpa\n",
    "from monpa import utils\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "# os.chdir('/Users/arthur/Desktop/Event_Driven_Stock_Prediction_using_Deep_Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75461f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "monpa.load_userdict('./input/stockName_abbr.txt')\n",
    "\n",
    "with open('./input/stockName_abbr.txt') as data_file:\n",
    "    stockName = data_file.readlines()\n",
    "\n",
    "stockNameAbbr = []\n",
    "for i in stockName:\n",
    "    stockNameAbbr.append(i.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2873f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這個function用來將字串以  **正則化**  處理去除中文字元以外的字元\n",
    "def clearSentence(sentence):\n",
    "    return re.sub(r'[^\\u4e00-\\u9fa5]+', '', sentence)\n",
    "#     sen = re.sub('-KY', '', sentence)\n",
    "#     return re.sub('\\*', '', sen)\n",
    "\n",
    "def splitSentence(content):\n",
    "    sentence_list = utils.short_sentence(content)\n",
    "    tokenStr = []\n",
    "    for sentence in sentence_list:\n",
    "#         sentence = clearSentence(sentence)\n",
    "        tokens = monpa.cut(sentence)\n",
    "        \n",
    "        ## 先斷完詞後，再看看是不是在stockNameAbbr裡面，如果不是的話就clearSentence(token)\n",
    "        tokensClear = []\n",
    "        for token in tokens:\n",
    "            if token in stockNameAbbr:\n",
    "                tokensClear.append(token)\n",
    "                continue\n",
    "            else:\n",
    "                token = clearSentence(token)\n",
    "                if token != '':\n",
    "                    tokensClear.append(clearSentence(token))\n",
    "        ##\n",
    "        \n",
    "        tokenStr += tokensClear\n",
    "    return tokenStr\n",
    "\n",
    "def find_key(dic, value):\n",
    "    for keys, values in dic.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "        if value == values:\n",
    "            return(keys)\n",
    "            break\n",
    "\n",
    "def trend(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    elif value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0964138f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'世芯' in stockNameAbbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a58d0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20131has already existed\n",
      "20132has already existed\n",
      "20133has already existed\n",
      "20134has already existed\n",
      "20135has already existed\n",
      "20136has already existed\n",
      "20137has already existed\n",
      "20138has already existed\n",
      "20139has already existed\n",
      "201310has already existed\n",
      "201311has already existed\n",
      "201312has already existed\n",
      "20141has already existed\n",
      "20142has already existed\n",
      "20143has already existed\n",
      "20144has already existed\n",
      "20145has already existed\n",
      "20146has already existed\n",
      "20147has already existed\n",
      "20148has already existed\n",
      "20149has already existed\n",
      "201410has already existed\n",
      "201411has already existed\n",
      "201412has already existed\n",
      "20151has already existed\n",
      "20152has already existed\n",
      "20153has already existed\n",
      "20154has already existed\n",
      "20155has already existed\n",
      "20156has already existed\n",
      "20157has already existed\n",
      "20158has already existed\n",
      "20159has already existed\n",
      "201510has already existed\n",
      "201511has already existed\n",
      "201512has already existed\n",
      "20161has already existed\n",
      "20162has already existed\n",
      "20163has already existed\n",
      "20164has already existed\n",
      "20165has already existed\n",
      "20166has already existed\n",
      "20167has already existed\n",
      "20168has already existed\n",
      "20169has already existed\n",
      "201610has already existed\n",
      "201611has already existed\n",
      "201612has already existed\n",
      "20171has already existed\n",
      "20172has already existed\n",
      "20173has already existed\n",
      "2017 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/monpa/crf_layer.py:374: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /Users/distiller/project/pytorch/aten/src/ATen/native/TensorCompare.cpp:333.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 5\n",
      "2017 6\n",
      "2017 7\n",
      "2017 8\n",
      "2017 9\n",
      "2017 10\n",
      "2017 11\n",
      "2017 12\n",
      "2018 1\n",
      "2018 2\n",
      "2018 3\n",
      "2018 4\n",
      "2018 5\n",
      "2018 6\n",
      "2018 7\n",
      "2018 8\n",
      "2018 9\n",
      "2018 10\n",
      "2018 11\n",
      "Warning: lengths ['The World’s biggest blockchain summit is coming to Taiwan next yearBlockchain applications are on the rise. World Blockchain Summit (WBS) will partner with New Taipei City Government to hold a summit'] (words) and\n",
      " [] (pos tags) mismatch\n",
      "2018 12\n",
      "Warning: lengths ['e 5 listed companies in the group hit limit down. Tatung Group had NTD10bn market value wiped out in one day.Affected by the poor market conditions of the panel', ',', ' CPT was unable to repay its debts and'] (words) and\n",
      " ['COMMACATEGORY', 'ORG'] (pos tags) mismatch\n",
      "Warning: lengths ['nancial Development Action Plan', '”', ' adopted by the Executive Yuan this year has listed fintech as one of the important strategies. Taiwan is one of the first countries in the world to set up a financial'] (words) and\n",
      " ['FW', 'FW'] (pos tags) mismatch\n",
      "Warning: lengths ['will follow IFRS17, or other financial issues such as whether not to use MtM (mark-to-market) to evaluate debt will be considered as well.In addition to the life insurance M&A, Lin said that the bank'] (words) and\n",
      " [] (pos tags) mismatch\n",
      "Warning: lengths ['Plant in Taiwan for the production. The Yangmei Plant has also expanded the 5” production capacity to 150 thousand pieces. Therefore', ',', ' the relocation will not affect existing customers and orders.Afte'] (words) and\n",
      " ['COMMACATEGORY', 'FW'] (pos tags) mismatch\n",
      "Warning: lengths ['e banned Huawei from providing equipment. In Taiwan', ',', ' National Communications Commission(NCC) banned Huawei’s cell site in 2013.As for now,', ' Koo said there is no precise law to regulate bank\"s security'] (words) and\n",
      " ['COMMACATEGORY', 'ORG', 'FW'] (pos tags) mismatch\n",
      "Warning: lengths ['name of foreign investors and acquire SMCT. Money laundering is involved as well. Sun asked the office to conduct a thorough ', 'investigation to see whether the unidentified group has violated the money'] (words) and\n",
      " ['FW'] (pos tags) mismatch\n",
      "20191has already existed\n",
      "20192has already existed\n",
      "20193has already existed\n",
      "20194has already existed\n",
      "20195has already existed\n",
      "20196has already existed\n",
      "20197has already existed\n",
      "20198has already existed\n",
      "2019 9\n",
      "2019 10\n",
      "2019 11\n",
      "2019 12\n",
      "20201has already existed\n",
      "2020 2\n",
      "2020 3\n",
      "2020 4\n",
      "2020 5\n",
      "2020 6\n",
      "2020 7\n",
      "2020 8\n",
      "2020 9\n",
      "2020 10\n",
      "2020 11\n",
      "2020 12\n",
      "20211has already existed\n",
      "2021 2\n",
      "2021 3\n",
      "2021 4\n",
      "2021 5\n",
      "2021 6\n",
      "2021 7\n",
      "2021 8\n",
      "2021 9\n",
      "2021 10\n",
      "2021 11\n",
      "2021 12\n"
     ]
    }
   ],
   "source": [
    "for i in range(2013, 2022):\n",
    "    for j in range(1,13):\n",
    "        try:\n",
    "            pd.read_csv('./input/EB_SegAndRawNews/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "            print(str(i) + str(j) + 'has already existed')\n",
    "        except:\n",
    "            print(i, j)\n",
    "            try:\n",
    "                df_news_1 = pd.read_csv('./input/RawData/'+str(i)+str(j)+'1_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news_2 = pd.read_csv('./input/RawData/'+str(i)+str(j)+'2_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news = pd.concat([df_news_1, df_news_2]).reset_index(drop=True)\n",
    "                df_news.date = pd.to_datetime(df_news.date).dt.date\n",
    "                \n",
    "#                 for k in range(df_news.shape[0]):\n",
    "#                     try:\n",
    "#                         df_news.loc[k, 'title_score'] = calSen(df_news.loc[k, 'title'])\n",
    "#                     except:\n",
    "#                         df_news.loc[k, 'title_score'] = 0\n",
    "#                     try:\n",
    "#                         df_news.loc[k, 'content_score'] = calSen(df_news.loc[k, 'content'])\n",
    "#                     except:\n",
    "#                         df_news.loc[k, 'content_score'] = 0\n",
    "#                     if (k % 100) == 0:\n",
    "#                         print(k, ' / ', df_news.shape[0])\n",
    "                df_news['all'] = df_news['title']+df_news['content']\n",
    "                df_news['all'] = df_news['all'].fillna('')\n",
    "                df_news['segmented'] = df_news['all'].apply(lambda x : splitSentence(x))\n",
    "                df_news = df_news[['date', 'all', 'segmented']] # should add 'all'!!!!!\n",
    "\n",
    "                df_news.to_csv('./input/EB_SegAndRawNews/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "            except:\n",
    "                print('Oh! We lost '+ str(i) + str(j))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f333a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
