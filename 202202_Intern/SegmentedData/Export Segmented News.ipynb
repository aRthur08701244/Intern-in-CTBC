{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42d5961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "已找到 model檔。Found model file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import datetime\n",
    "import monpa\n",
    "from monpa import utils\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "os.chdir('/Users/kmes8/Desktop/CTBC')\n",
    "# os.chdir('/Users/arthur/Desktop/Event_Driven_Stock_Prediction_using_Deep_Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2873f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我們從stopwords_zh.txt這個檔案中匯入繁體中文的停用詞\n",
    "with open('./stopwords_zh.txt', 'r', encoding='utf8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "file.close()\n",
    "\n",
    "with open('./NTUSD_negative_unicode.txt', 'r', encoding='utf8') as file:\n",
    "    nega_words = file.read().splitlines()\n",
    "file.close()\n",
    "\n",
    "with open('./NTUSD_positive_unicode.txt', 'r', encoding='utf8') as file:\n",
    "    posi_words = file.read().splitlines()\n",
    "file.close()\n",
    "\n",
    "# 這個function用來將字串以  **正則化**  處理去除中文字元以外的字元\n",
    "def clearSentence(sentence):\n",
    "    return re.sub(r'[^\\u4e00-\\u9fa5]+', '', sentence)\n",
    "\n",
    "def splitSentence(content):\n",
    "    sentence_list = utils.short_sentence(content)\n",
    "    tokenStr = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence = clearSentence(sentence)\n",
    "        tokens = monpa.cut(sentence)\n",
    "        tokenStr += tokens\n",
    "    return tokenStr\n",
    "\n",
    "def calSen(content):\n",
    "    sentence_list = utils.short_sentence(content)\n",
    "    tokenStr = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence = clearSentence(sentence)\n",
    "        tokens = monpa.cut(sentence)\n",
    "        tokenStr += tokens\n",
    "        \n",
    "    senScore = 0\n",
    "    for i in tokenStr:\n",
    "        if i in posi_words:\n",
    "            senScore += 1\n",
    "        elif i in nega_words:\n",
    "            senScore -= 1\n",
    "    return senScore\n",
    "\n",
    "def find_key(dic, value):\n",
    "    for keys, values in dic.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "        if value == values:\n",
    "            return(keys)\n",
    "            break\n",
    "\n",
    "def trend(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    elif value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58d0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmes8\\anaconda3\\lib\\site-packages\\monpa\\crf_layer.py:374: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorCompare.cpp:333.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 2\n",
      "2015 3\n",
      "2015 4\n",
      "2015 5\n",
      "2015 6\n",
      "2015 7\n",
      "2015 8\n",
      "2015 9\n",
      "2015 10\n",
      "2015 11\n",
      "2015 12\n",
      "2016 1\n",
      "2016 2\n",
      "2016 3\n",
      "2016 4\n",
      "2016 5\n",
      "2016 6\n",
      "2016 7\n",
      "2016 8\n",
      "2016 9\n",
      "2016 10\n",
      "2016 11\n",
      "2016 12\n",
      "2017 1\n",
      "2017 2\n",
      "2017 3\n",
      "2017 4\n",
      "2017 5\n",
      "2017 6\n",
      "2017 7\n",
      "2017 8\n",
      "2017 9\n",
      "2017 10\n",
      "2017 11\n",
      "2017 12\n",
      "2018 1\n",
      "2018 2\n",
      "2018 3\n",
      "2018 4\n",
      "2018 5\n",
      "2018 6\n",
      "2018 7\n",
      "2018 8\n",
      "2018 9\n",
      "2018 10\n",
      "2018 11\n",
      "2018 12\n",
      "2019 1\n",
      "2019 2\n",
      "2019 3\n",
      "2019 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(2013, 2022):\n",
    "    for j in range(1,13):\n",
    "        try:\n",
    "            pd.read_csv('./SegmentedData/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "        except:\n",
    "            print(i, j)\n",
    "            try:\n",
    "                df_news_1 = pd.read_csv('./RawData/'+str(i)+str(j)+'1_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news_2 = pd.read_csv('./RawData/'+str(i)+str(j)+'2_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news = pd.concat([df_news_1, df_news_2]).reset_index(drop=True)\n",
    "                df_news.date = pd.to_datetime(df_news.date).dt.date\n",
    "                \n",
    "#                 for k in range(df_news.shape[0]):\n",
    "#                     try:\n",
    "#                         df_news.loc[k, 'title_score'] = calSen(df_news.loc[k, 'title'])\n",
    "#                     except:\n",
    "#                         df_news.loc[k, 'title_score'] = 0\n",
    "#                     try:\n",
    "#                         df_news.loc[k, 'content_score'] = calSen(df_news.loc[k, 'content'])\n",
    "#                     except:\n",
    "#                         df_news.loc[k, 'content_score'] = 0\n",
    "#                     if (k % 100) == 0:\n",
    "#                         print(k, ' / ', df_news.shape[0])\n",
    "                df_news['all'] = df_news['title']+df_news['content']\n",
    "                df_news['all'] = df_news['all'].fillna('')\n",
    "                df_news['segmented'] = df_news['all'].apply(lambda x : splitSentence(x))\n",
    "                df_news = df_news[['date', 'segmented']]\n",
    "\n",
    "                df_news.to_csv('./SegmentedData/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "            except:\n",
    "                print('Oh! We lost '+ str(i) + str(j))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ee56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
