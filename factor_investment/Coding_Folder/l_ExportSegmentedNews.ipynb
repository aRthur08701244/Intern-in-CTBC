{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42d5961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "已找到 model檔。Found model file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import datetime\n",
    "import monpa\n",
    "from monpa import utils\n",
    "import re\n",
    "import numpy as np\n",
    "import copy\n",
    "os.chdir('/Users/arthur/Desktop/factor_investment')\n",
    "# os.chdir('/Users/arthur/Desktop/Event_Driven_Stock_Prediction_using_Deep_Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2873f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我們從stopwords_zh.txt這個檔案中匯入繁體中文的停用詞\n",
    "with open('./202202_Intern/stopwords_zh.txt', 'r') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "file.close()\n",
    "\n",
    "with open('./202202_Intern/NTUSD_negative_unicode.txt', 'r') as file:\n",
    "    nega_words = file.read().splitlines()\n",
    "file.close()\n",
    "\n",
    "with open('./202202_Intern/NTUSD_positive_unicode.txt', 'r') as file:\n",
    "    posi_words = file.read().splitlines()\n",
    "file.close()\n",
    "\n",
    "# 這個function用來將字串以  **正則化**  處理去除中文字元以外的字元\n",
    "def clearSentence(sentence):\n",
    "    return re.sub(r'[^\\u4e00-\\u9fa5]+', '', sentence)\n",
    "\n",
    "def splitSentence(content):\n",
    "    sentence_list = utils.short_sentence(content)\n",
    "    tokenStr = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence = clearSentence(sentence)\n",
    "        tokens = monpa.cut(sentence)\n",
    "        tokenStr += tokens\n",
    "    return tokenStr\n",
    "\n",
    "def calSen(content):\n",
    "    sentence_list = utils.short_sentence(content)\n",
    "    tokenStr = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence = clearSentence(sentence)\n",
    "        tokens = monpa.cut(sentence)\n",
    "        tokenStr += tokens\n",
    "        \n",
    "    senScore = 0\n",
    "    for i in tokenStr:\n",
    "        if i in posi_words:\n",
    "            senScore += 1\n",
    "        elif i in nega_words:\n",
    "            senScore -= 1\n",
    "    return senScore\n",
    "\n",
    "def find_key(dic, value):\n",
    "    for keys, values in dic.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "        if value == values:\n",
    "            return(keys)\n",
    "            break\n",
    "\n",
    "def trend(value):\n",
    "    if value > 0:\n",
    "        return 1\n",
    "    elif value < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92db5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monpa.load_userdict(\"./202202_Intern/stock_abbr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a58d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2013, 2015):\n",
    "    for j in range(1,13):\n",
    "        try:\n",
    "            pd.read_csv('./202202_Intern/SegmentedData/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "        except:\n",
    "            print(i, j)\n",
    "            try:\n",
    "                df_news_1 = pd.read_csv('./202202_Intern/RawData/'+str(i)+str(j)+'1_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news_2 = pd.read_csv('./202202_Intern/RawData/'+str(i)+str(j)+'2_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news = pd.concat([df_news_1, df_news_2]).reset_index(drop=True)\n",
    "                df_news.date = pd.to_datetime(df_news.date).dt.date\n",
    "                \n",
    "#                 for k in range(df_news.shape[0]):\n",
    "#                     try:\n",
    "#                         df_news.loc[k, 'title_score'] = calSen(df_news.loc[k, 'title'])\n",
    "#                     except:\n",
    "#                         df_news.loc[k, 'title_score'] = 0\n",
    "#                     try:\n",
    "#                         df_news.loc[k, 'content_score'] = calSen(df_news.loc[k, 'content'])\n",
    "#                     except:\n",
    "#                         df_news.loc[k, 'content_score'] = 0\n",
    "#                     if (k % 100) == 0:\n",
    "#                         print(k, ' / ', df_news.shape[0])\n",
    "                df_news['all'] = df_news['title']+df_news['content']\n",
    "                df_news['all'] = df_news['all'].fillna('')\n",
    "                df_news['segmented'] = df_news['all'].apply(lambda x : splitSentence(x))\n",
    "                df_news = df_news[['date', 'segmented']] # should add 'all'!!!!!\n",
    "\n",
    "                df_news.to_csv('./202202_Intern/SegmentedData/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "            except:\n",
    "                print('Oh! We lost '+ str(i) + str(j))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517694db",
   "metadata": {},
   "source": [
    "# 上面有些沒有對「正確的股票簡稱」做正確的斷詞，在1_transformed_news作更正。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741dab7",
   "metadata": {},
   "source": [
    "# 合併原始新聞＋斷詞後的新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af8bd70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2013, 2022):\n",
    "    for j in range(1,13):\n",
    "        try:\n",
    "            pd.read_csv('./202202_Intern/SegAndRawData/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "        except:\n",
    "            print(i, j)\n",
    "            try:\n",
    "                df_news_1 = pd.read_csv('./202202_Intern/RawData/'+str(i)+str(j)+'1_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news_2 = pd.read_csv('./202202_Intern/RawData/'+str(i)+str(j)+'2_cnyesnews.csv')[['date', 'title', 'content']]\n",
    "                df_news = pd.concat([df_news_1, df_news_2]).reset_index(drop=True)\n",
    "                df_news.date = pd.to_datetime(df_news.date).dt.date\n",
    "                \n",
    "                df_news['all'] = df_news['title']+df_news['content']\n",
    "                df_news['all'] = df_news['all'].fillna('')\n",
    "#                 df_news['segmented'] = df_news['all'].apply(lambda x : splitSentence(x))\n",
    "#                 df_news = df_news[['date', 'segmented']]\n",
    "\n",
    "                df_segnews = pd.read_csv('./202202_Intern/SegmentedData/'+str(i)+str(j)+'_cnyesnews.csv')\n",
    "                df_news['segmented'] = df_segnews['segmented']\n",
    "                df_news = df_news[['date', 'all', 'segmented']]\n",
    "\n",
    "                df_news.to_csv('./202202_Intern/SegAndRawData/'+str(i)+str(j)+'_cnyesnews.csv', index=False)\n",
    "            except:\n",
    "                print('Oh! We lost '+ str(i) + str(j))\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f024c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
